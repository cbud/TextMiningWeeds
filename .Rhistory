slice_max(order_by=prop, n=12)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop, fill=weed_searched)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good")%>% group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french")%>%  group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=8)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop, fill=weed_searched)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good")%>% group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french")%>%  group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=5)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop, fill=weed_searched)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good")%>% group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french")%>%  group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=1)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop, fill=weed_searched)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good")%>% group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french")%>%  group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=5)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop, fill=weed_searched)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good")%>% group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french")%>%  group_by(weed_searched) %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=10)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop, fill=weed_searched)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
# ngrams 1 proportions ----------------------------------------------------
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good") %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french") %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=10)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good") %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french") %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=15)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good") %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french") %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=20)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good") %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french") %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=bad/good) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=30)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
# ngrams 1 proportions ----------------------------------------------------
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good") %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french") %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=good/bad) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=30)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium perenne","avena fatua","chenopdium album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good") %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french") %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=good/bad) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=35)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
# ngrams 1 proportions ----------------------------------------------------
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium","perenne","avena", "fatua","chenopodium","album"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good") %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french") %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=good/bad) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=35)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
weed_papers <- read_csv("Small Data Files/weeds_paper_chris_assess.csv")
weed_papers$combined <- paste(weed_papers$title, weed_papers$description, weed_papers$authkeywords, weed_papers$publicationName)
weed_papers$combined <- weed_papers$combined%>% str_replace_all(., "[[:punct:]]", "") %>% tolower()
species_remove<-data.frame(word = c("lolium","perenne","avena", "fatua","chenopodium","album", "de", "la", "oat", "italian"))
weed_paper_words<-unnest_tokens(weed_papers,word,combined)
weed_paper_words<- weed_paper_words %>% anti_join(species_remove, by = "word")
word_freq_good <- weed_paper_words %>% filter(GoodBad=="good") %>% count(word, sort = TRUE) %>%rename("good"=n)
word_freq_bad <-  weed_paper_words %>% filter(GoodBad !="good"& GoodBad != "french") %>% count(word, sort = TRUE) %>%rename("bad"=n)
word_freq_goodbad<-left_join(word_freq_bad,word_freq_good)
word_prop<- word_freq_goodbad %>% mutate(prop=good/bad) %>% arrange(-prop)
word_prop %>%
slice_max(order_by=prop, n=35)  %>%  mutate(word = reorder(word, prop))%>%
ggplot(aes(word, prop)) + geom_bar(position="stack", stat="identity") + xlab(NULL) + coord_flip()
#Chris Buddenhagen 2022-07-14
#Bibliometric analysis
#Vignette source information: https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html #see also package help
# Load packages
library(bib2df)
library(bibliometrix)
library(tidyverse)
library(rscopus)
library(readr)
#Not synching large files on github
#dir()
# [1] "Large Data Files"      "Outputs"               "README.md"             "Small Data Files"
# [5] "TextMiningWeeds.Rproj" "Weed_text_mining.r"
#Note it looks like it is better to download from one recognized source through an API for bibliometrix package
#GW<-convert2df(path2file, dbsource = "scopus", format = "endnote")
#results <- biblioAnalysis(GW, sep = ";")
#Note it looks like it is better to download from one recognized source through an API for bibliometrix package
#GW<-convert2df(path2file, dbsource = "scopus", format = "endnote")
#path2file<-"Small Data Files/Good weed references.bib"
#GW<-bib2df(path2file, separate_names = TRUE)
#path2file<-"Small Data Files/Some problem reference examples.bib"
#BW<-bib2df(path2file, separate_names = TRUE)
#import data
good_papers<- bib2df("Small Data Files/Good weed references.bib", separate_names = TRUE)%>%
mutate(status="good")
bad_papers<- bib2df("Small Data Files/Some problem reference examples.bib", separate_names = TRUE)%>%
mutate(status="bad")
before_clean <- bind_rows(good_papers, bad_papers)
#adds lowercase title, abstract, and keywords to new column
before_clean$combined <- paste(tolower(before_clean$TITLE), tolower(before_clean$ABSTRACT), tolower(before_clean$KEYWORDS), tolower(before_clean$JOURNAL), (before_clean$BOOKTITLE), (before_clean$CHAPTER))
before_clean$TITLE <- before_clean$TITLE %>% str_replace_all(., "[[:punct:]]", "")
good_papers$TITLE <- good_papers$TITLE %>% str_replace_all(., "[[:punct:]]", "")
#remove duplicates
cleaning<-before_clean %>%
distinct(paste(tolower(TITLE)), .keep_all=TRUE) %>%
select(-"paste(tolower(TITLE))") %>% #remove temporary column
mutate(keep="good") #create keep column
#remove duplicates another way
#removed_weeds<- filter(before_clean,duplicated(paste(tolower(before_clean$TITLE)))) %>% mutate(keep="duplicate")
#add duplicates back in, with keep = duplicate
cleaning<- before_clean %>% left_join(cleaning, by = names(.))
cleaning$keep[is.na(cleaning$keep)] <- "duplicate"
#create function to remove rows based on input words
remove_word <- function(input) {
for (i in 1:length(input)) {
removed <- cleaning[grep(input[i], cleaning$combined), ]  #subset rows based on input word
removed <- removed %>% #filter(keep!="duplicate") %>%
mutate(keep = paste0(ifelse(keep == "good", "", paste0(keep, ", ")), input[i])) #replace keep with input word + any other exclusion words
cleaning <-
cleaning %>% anti_join(removed, by = "TITLE") %>% full_join(removed, by = names(.)) #adds subset rows back onto main weed paper list
}
return(cleaning)
}
#test
# after_clean<-remove_word(list("insect pests", "medicin", "medical", "cancer",  "carnivore", "domesticated", "folk", "herbal", "essential oils"))
#
# to_add <- cleaning[grep("weeds", cleaning$combined), ]  #subset rows based on input word
# to_add
# to_add <- to_add %>% filter(keep!="good")%>%
#   mutate(keep = "ok") #replace keep with input word + any other exclusion words
# to_add
# after_clean <-
#   #to_add %>% anti_join(to_add, by = "TITLE") %>% full_join(to_add, by = names(.)) #adds subset rows back onto main weed paper list
# after_clean %>% anti_join(to_add, by = c("TITLE")) %>% full_join(to_add, by = c("TITLE","keep"))
keep_words <- function(input) {
for (i in 1:length(input)) {
to_add <- cleaning[grep(input[i], tolower(cleaning$JOURNAL)), ]  #subset rows based on input word
to_add <- to_add %>% filter(!(grepl("duplicate",keep))) %>%
mutate(keep = "include") #replace keep with input word + any other exclusion words
after_clean <-
after_clean %>% anti_join(to_add, by = "TITLE") %>% full_join(to_add, by = names(.)) #adds subset rows back onto main weed paper list
}
return(after_clean)
}
#uses the function to remove papers that contain any of the input words. You can run this bit separately with different terms.
after_clean<-remove_word(list("insect pest","pests", "wildlife","medicin", "cover crop","medical", "cancer",  "carnivore", "domesticated", "folk", "herbal", "essential oils", "weed"))
after_clean<-keep_words(list("herbicide"))
#possible others: folk, chemistry, herbal, essential oils
#calculates erroneous removals
missed_bad <- after_clean %>% filter(keep=="good" | keep=="include") %>% anti_join(good_papers)
excluded_good <- after_clean %>% filter(keep!="good" & keep!="include") %>% merge(good_papers)
#shows summary of papers excluded
a <- after_clean%>%
group_by(keep) %>%
summarise(n()) %>%
arrange(-.[2]) #arrange by 2nd column, descending
sum(a$`n()`)
a
paste(nrow(missed_bad), "missed bad papers")
paste(nrow(excluded_good), "excluded good papers")
#Chris Buddenhagen 2022-07-14
#Bibliometric analysis
#Vignette source information: https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html #see also package help
# Load packages
library(bib2df)
library(bibliometrix)
library(tidyverse)
library(rscopus)
library(readr)
#Not synching large files on github
#dir()
# [1] "Large Data Files"      "Outputs"               "README.md"             "Small Data Files"
# [5] "TextMiningWeeds.Rproj" "Weed_text_mining.r"
#Note it looks like it is better to download from one recognized source through an API for bibliometrix package
#GW<-convert2df(path2file, dbsource = "scopus", format = "endnote")
#results <- biblioAnalysis(GW, sep = ";")
#Note it looks like it is better to download from one recognized source through an API for bibliometrix package
#GW<-convert2df(path2file, dbsource = "scopus", format = "endnote")
#path2file<-"Small Data Files/Good weed references.bib"
#GW<-bib2df(path2file, separate_names = TRUE)
#path2file<-"Small Data Files/Some problem reference examples.bib"
#BW<-bib2df(path2file, separate_names = TRUE)
#import data
good_papers<- bib2df("Small Data Files/Good weed references.bib", separate_names = TRUE)%>%
mutate(status="good")
bad_papers<- bib2df("Small Data Files/Some problem reference examples.bib", separate_names = TRUE)%>%
mutate(status="bad")
before_clean <- bind_rows(good_papers, bad_papers)
#adds lowercase title, abstract, and keywords to new column
before_clean$combined <- paste(tolower(before_clean$TITLE), tolower(before_clean$ABSTRACT), tolower(before_clean$KEYWORDS), tolower(before_clean$JOURNAL), (before_clean$BOOKTITLE), (before_clean$CHAPTER))
before_clean$TITLE <- before_clean$TITLE %>% str_replace_all(., "[[:punct:]]", "")
good_papers$TITLE <- good_papers$TITLE %>% str_replace_all(., "[[:punct:]]", "")
#remove duplicates
cleaning<-before_clean %>%
distinct(paste(tolower(TITLE)), .keep_all=TRUE) %>%
select(-"paste(tolower(TITLE))") %>% #remove temporary column
mutate(keep="good") #create keep column
#remove duplicates another way
#removed_weeds<- filter(before_clean,duplicated(paste(tolower(before_clean$TITLE)))) %>% mutate(keep="duplicate")
#add duplicates back in, with keep = duplicate
cleaning<- before_clean %>% left_join(cleaning, by = names(.))
cleaning$keep[is.na(cleaning$keep)] <- "duplicate"
#create function to remove rows based on input words
remove_word <- function(input) {
for (i in 1:length(input)) {
removed <- cleaning[grep(input[i], cleaning$combined), ]  #subset rows based on input word
removed <- removed %>% #filter(keep!="duplicate") %>%
mutate(keep = paste0(ifelse(keep == "good", "", paste0(keep, ", ")), input[i])) #replace keep with input word + any other exclusion words
cleaning <-
cleaning %>% anti_join(removed, by = "TITLE") %>% full_join(removed, by = names(.)) #adds subset rows back onto main weed paper list
}
return(cleaning)
}
#test
# after_clean<-remove_word(list("insect pests", "medicin", "medical", "cancer",  "carnivore", "domesticated", "folk", "herbal", "essential oils"))
#
# to_add <- cleaning[grep("weeds", cleaning$combined), ]  #subset rows based on input word
# to_add
# to_add <- to_add %>% filter(keep!="good")%>%
#   mutate(keep = "ok") #replace keep with input word + any other exclusion words
# to_add
# after_clean <-
#   #to_add %>% anti_join(to_add, by = "TITLE") %>% full_join(to_add, by = names(.)) #adds subset rows back onto main weed paper list
# after_clean %>% anti_join(to_add, by = c("TITLE")) %>% full_join(to_add, by = c("TITLE","keep"))
keep_words <- function(input) {
for (i in 1:length(input)) {
to_add <- cleaning[grep(input[i], tolower(cleaning$JOURNAL)), ]  #subset rows based on input word
to_add <- to_add %>% filter(!(grepl("duplicate",keep))) %>%
mutate(keep = "include") #replace keep with input word + any other exclusion words
after_clean <-
after_clean %>% anti_join(to_add, by = "TITLE") %>% full_join(to_add, by = names(.)) #adds subset rows back onto main weed paper list
}
return(after_clean)
}
#uses the function to remove papers that contain any of the input words. You can run this bit separately with different terms.
after_clean<-remove_word(list("insect pest","pests", "wildlife","medicin", "cover crop","medical", "cancer",  "carnivore", "domesticated", "folk", "herbal", "essential oils"))
after_clean<-keep_words(list("herbicide"))
#possible others: folk, chemistry, herbal, essential oils
#calculates erroneous removals
missed_bad <- after_clean %>% filter(keep=="good" | keep=="include") %>% anti_join(good_papers)
excluded_good <- after_clean %>% filter(keep!="good" & keep!="include") %>% merge(good_papers)
#shows summary of papers excluded
a <- after_clean%>%
group_by(keep) %>%
summarise(n()) %>%
arrange(-.[2]) #arrange by 2nd column, descending
sum(a$`n()`)
a
paste(nrow(missed_bad), "missed bad papers")
paste(nrow(excluded_good), "excluded good papers")
#Chris Buddenhagen 2022-07-14
#Bibliometric analysis
#Vignette source information: https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html #see also package help
# Load packages
library(bib2df)
library(bibliometrix)
library(tidyverse)
library(rscopus)
library(readr)
#Not synching large files on github
#dir()
# [1] "Large Data Files"      "Outputs"               "README.md"             "Small Data Files"
# [5] "TextMiningWeeds.Rproj" "Weed_text_mining.r"
#Note it looks like it is better to download from one recognized source through an API for bibliometrix package
#GW<-convert2df(path2file, dbsource = "scopus", format = "endnote")
#results <- biblioAnalysis(GW, sep = ";")
#Note it looks like it is better to download from one recognized source through an API for bibliometrix package
#GW<-convert2df(path2file, dbsource = "scopus", format = "endnote")
#path2file<-"Small Data Files/Good weed references.bib"
#GW<-bib2df(path2file, separate_names = TRUE)
#path2file<-"Small Data Files/Some problem reference examples.bib"
#BW<-bib2df(path2file, separate_names = TRUE)
#import data
good_papers<- bib2df("Small Data Files/Good weed references.bib", separate_names = TRUE)%>%
mutate(status="good")
bad_papers<- bib2df("Small Data Files/Some problem reference examples.bib", separate_names = TRUE)%>%
mutate(status="bad")
before_clean <- bind_rows(good_papers, bad_papers)
#adds lowercase title, abstract, and keywords to new column
before_clean$combined <- paste(tolower(before_clean$TITLE), tolower(before_clean$ABSTRACT), tolower(before_clean$KEYWORDS), tolower(before_clean$JOURNAL), (before_clean$BOOKTITLE), (before_clean$CHAPTER))
before_clean$TITLE <- before_clean$TITLE %>% str_replace_all(., "[[:punct:]]", "")
good_papers$TITLE <- good_papers$TITLE %>% str_replace_all(., "[[:punct:]]", "")
#remove duplicates
cleaning<-before_clean %>%
distinct(paste(tolower(TITLE)), .keep_all=TRUE) %>%
select(-"paste(tolower(TITLE))") %>% #remove temporary column
mutate(keep="good") #create keep column
#remove duplicates another way
#removed_weeds<- filter(before_clean,duplicated(paste(tolower(before_clean$TITLE)))) %>% mutate(keep="duplicate")
#add duplicates back in, with keep = duplicate
cleaning<- before_clean %>% left_join(cleaning, by = names(.))
cleaning$keep[is.na(cleaning$keep)] <- "duplicate"
#create function to remove rows based on input words
remove_word <- function(input) {
for (i in 1:length(input)) {
removed <- cleaning[grep(input[i], cleaning$combined), ]  #subset rows based on input word
removed <- removed %>% #filter(keep!="duplicate") %>%
mutate(keep = paste0(ifelse(keep == "good", "", paste0(keep, ", ")), input[i])) #replace keep with input word + any other exclusion words
cleaning <-
cleaning %>% anti_join(removed, by = "TITLE") %>% full_join(removed, by = names(.)) #adds subset rows back onto main weed paper list
}
return(cleaning)
}
keep_words <- function(input) {
for (i in 1:length(input)) {
to_add <- cleaning[grep(input[i], tolower(cleaning$JOURNAL)), ]  #subset rows based on input word
to_add <- to_add %>% filter(!(grepl("duplicate",keep))) %>%
mutate(keep = "include") #replace keep with input word + any other exclusion words
after_clean <-
after_clean %>% anti_join(to_add, by = "TITLE") %>% full_join(to_add, by = names(.)) #adds subset rows back onto main weed paper list
}
return(after_clean)
}
#uses the function to remove papers that contain any of the input words. You can run this bit separately with different terms.
after_clean<-remove_word(list("insect pest","pests", "wildlife","medicin", "cover crop","medical", "cancer",  "carnivore", "domesticated", "folk", "herbal", "essential oils"))
after_clean<-keep_words(list("herbicide"))
#possible others: folk, chemistry, herbal, essential oils
#calculates erroneous removals
missed_bad <- after_clean %>% filter(keep=="good" | keep=="include") %>% anti_join(good_papers)
excluded_good <- after_clean %>% filter(keep!="good" & keep!="include") %>% merge(good_papers)
#shows summary of papers excluded
a <- after_clean%>%
group_by(keep) %>%
summarise(n()) %>%
arrange(-.[2]) #arrange by 2nd column, descending
sum(a$`n()`)
a
paste(nrow(missed_bad), "missed bad papers")
paste(nrow(excluded_good), "excluded good papers")
#Chris Buddenhagen 2022-07-14
#Bibliometric analysis
#Vignette source information: https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html #see also package help
# Load packages
library(bib2df)
library(bibliometrix)
library(tidyverse)
library(rscopus)
library(readr)
#Not synching large files on github
#dir()
# [1] "Large Data Files"      "Outputs"               "README.md"             "Small Data Files"
# [5] "TextMiningWeeds.Rproj" "Weed_text_mining.r"
#Note it looks like it is better to download from one recognized source through an API for bibliometrix package
#GW<-convert2df(path2file, dbsource = "scopus", format = "endnote")
#results <- biblioAnalysis(GW, sep = ";")
#Note it looks like it is better to download from one recognized source through an API for bibliometrix package
#GW<-convert2df(path2file, dbsource = "scopus", format = "endnote")
#path2file<-"Small Data Files/Good weed references.bib"
#GW<-bib2df(path2file, separate_names = TRUE)
#path2file<-"Small Data Files/Some problem reference examples.bib"
#BW<-bib2df(path2file, separate_names = TRUE)
#import data
good_papers<- bib2df("Small Data Files/Good weed references.bib", separate_names = TRUE)%>%
mutate(status="good")
bad_papers<- bib2df("Small Data Files/Some problem reference examples.bib", separate_names = TRUE)%>%
mutate(status="bad")
before_clean <- bind_rows(good_papers, bad_papers)
#adds lowercase title, abstract, and keywords to new column
before_clean$combined <- paste(tolower(before_clean$TITLE), tolower(before_clean$ABSTRACT), tolower(before_clean$KEYWORDS), tolower(before_clean$JOURNAL), (before_clean$BOOKTITLE), (before_clean$CHAPTER))
before_clean$TITLE <- before_clean$TITLE %>% str_replace_all(., "[[:punct:]]", "")
good_papers$TITLE <- good_papers$TITLE %>% str_replace_all(., "[[:punct:]]", "")
#remove duplicates
cleaning<-before_clean %>%
distinct(paste(tolower(TITLE)), .keep_all=TRUE) %>%
select(-"paste(tolower(TITLE))") %>% #remove temporary column
mutate(keep="good") #create keep column
#remove duplicates another way
#removed_weeds<- filter(before_clean,duplicated(paste(tolower(before_clean$TITLE)))) %>% mutate(keep="duplicate")
#add duplicates back in, with keep = duplicate
cleaning<- before_clean %>% left_join(cleaning, by = names(.))
cleaning$keep[is.na(cleaning$keep)] <- "duplicate"
#create function to remove papers with certain words in the combined column
remove_word <- function(input) {
for (i in 1:length(input)) {
removed <- cleaning[grep(input[i], cleaning$combined), ]  #subset rows based on input word
removed <- removed %>% #filter(keep!="duplicate") %>%
mutate(keep = paste0(ifelse(keep == "good", "", paste0(keep, ", ")), input[i])) #replace keep with input word + any other exclusion words
cleaning <-
cleaning %>% anti_join(removed, by = "TITLE") %>% full_join(removed, by = names(.)) #adds subset rows back onto main weed paper list
}
return(cleaning)
}
#create function to keep papers with certain words in the journal name
keep_words <- function(input) {
for (i in 1:length(input)) {
to_add <- cleaning[grep(input[i], tolower(cleaning$JOURNAL)), ]  #subset rows based on input word
to_add <- to_add %>% filter(!(grepl("duplicate",keep))) %>%
mutate(keep = "include") #replace keep with input word + any other exclusion words
after_clean <-
after_clean %>% anti_join(to_add, by = "TITLE") %>% full_join(to_add, by = names(.)) #adds subset rows back onto main weed paper list
}
return(after_clean)
}
#runs the two functions above. You can run this bit separately with different terms.
after_clean<-remove_word(list("insect pest","pests", "wildlife","medicin", "cover crop","medical", "cancer",  "carnivore", "domesticated", "folk", "herbal", "essential oils"))
after_clean<-keep_words(list("herbicide"))
#possible others: folk, chemistry, herbal, essential oils
#calculates erroneous removals
missed_bad <- after_clean %>% filter(keep=="good" | keep=="include") %>% anti_join(good_papers)
excluded_good <- after_clean %>% filter(keep!="good" & keep!="include") %>% merge(good_papers)
#shows summary of papers excluded
after_clean%>%
group_by(keep) %>%
summarise(n()) %>%
arrange(-.[2]) #arrange by 2nd column, descending
#sum(a$`n()`) #sum of papers
paste(nrow(missed_bad), "missed bad papers")
paste(nrow(excluded_good), "excluded good papers")
